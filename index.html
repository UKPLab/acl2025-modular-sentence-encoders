<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Modular Sentence Encoders: Separating Language Specialization from Cross-Lingual Alignment</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Yongxin Huang<sup>1</sup>,</span>
                <span class="author-block">
                  Kexin Wang</a><sup>1</sup>,</span>
                  <span class="author-block">
                    Goran Glavaš</a><sup>2</sup>,</span>
                  <span class="author-block">
                    Iryna Gurevych<sup>1</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>UKP Lab, Technical University of Darmstadt <br> <sup>2</sup>Center for AI and Data Science, University of Würzburg<br>ACL 2025</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div> -->

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://aclanthology.org/2025.acl-long.108/" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon" style="color:white;">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span style="color:white;">Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/UKPLab/acl2025-modular-sentence-encoders" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon" style="color:white;">
                      <i class="fab fa-github"></i>
                    </span>
                    <span style="color:white;">Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2407.14878" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon" style="color:white;">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span style="color:white;">arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Motivation-->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
      <h2 class="title is-3 has-text-centered">Current multilingual sentence encoders face three key challenges.</h2>
      <h2 class="content has-text-justified">
<b>Curse of multilinguality</b>: Loss of Representation precision for individual languages due to parameter sharing. <br><br> 
Trade-off between <b>monolingual</b> and <b>cross-lingual</b> performance: Forcing cross-lingual alignment between non-isomorphic monolingual spaces can distort monolingual semantic structures. <br><br>  
Different cross-lingual tasks place <b>conflicting demands</b> on the representation space: 
<b>Cross-lingual transfer</b> (on a multilingual collection of monolingual tasks) benefits from high isomorphism between monolingual spaces (!= language-agnostic space).
<b>Cross-lingual semantic textual similarity (STS)</b> requires modelling fine-grained semantic similarity, from exact equivalence to complete mismatch.
<b>Bitext mining</b> requires low similarity between embeddings of non-translation pairs (even if they are similar). <br><br>  
      </h2>
<img src="static/images/sts_bitext_mining.png" width="80%"/>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multilingual sentence encoders (MSEs) are commonly obtained by training multilingual language models to map sentences from different languages into a shared semantic space. As such, they are subject to curse of multilinguality, a loss of monolingual representational accuracy due to parameter sharing. Another limitation of MSEs is the trade-off between different task performance: cross-lingual alignment training distorts the optimal monolingual structure of semantic spaces of individual languages, harming the utility of sentence embeddings in monolingual tasks; cross-lingual tasks, such as cross-lingual semantic similarity and zero-shot transfer for sentence classification, may also require conflicting cross-lingual alignment strategies. In this work, we address both issues by means of modular training of sentence encoders. We first train language-specific monolingual modules to mitigate negative interference between languages (i.e., the curse). We then align all non-English sentence embeddings to the English by training cross-lingual alignment adapters, preventing interference with monolingual specialization from the first step. We train the cross-lingual adapters with two different types of data to resolve the conflicting requirements of different cross-lingual tasks. Monolingual and cross-lingual results on semantic text similarity and relatedness, bitext mining and sentence classification show that our modular solution achieves better and more balanced performance across all the tasks compared to full-parameter training of monolithic multilingual sentence encoders, especially benefiting low-resource languages.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Method -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">We mitigate these issues through modularity.</h2>
        <img src="static/images/figure1.png" width="75%"/>
        <h2 class="content has-text-justified">
           To address the curse of multilinguality, our approach begins with a <b>monolingual specialization</b> step. We start with a pre-trained multilingual encoder, such as LaBSE or multilingual-e5, and adapt it to each language individually by training language-specific modules. 
           We first train the new <b>embedding layer</b> and a lightweight <b>language adapter</b> through language modelling on monolingual corpora. 
           Then we train a <b>sentence encoding (SE) adapter</b> for each language to produce high-quality sentence embeddings. 
           For this, we use paraphrase data in each target language, machine-translated from English. 
        </h2>
        <h2 class="content has-text-justified">
           In the <b>Cross-Lingual Alignment (CLA)</b> step, we align all other languages to English as a pivot language.
We train a <b>CLA adapter</b> on top of the monolingual modules. This design avoids overwriting the monolingual semantic structures. 
The CLA adapter is trained on the mixture of cross-lingual data paraphrase pairs (semantically similar but not equivalent), and parallel translation pairs. This allows the model to capture both cross-lingual fine-grained similarity and precise cross-lingual equivalence mappings.
        </h2>        
      </div>
    </div>
  </div>
</section>


<!-- Paper poster --> 
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/poster.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{huang-etal-2025-modular,
    title = "Modular Sentence Encoders: Separating Language Specialization from Cross-Lingual Alignment",
    author = "Huang, Yongxin  and
      Wang, Kexin  and
      Glava{\v{s}}, Goran  and
      Gurevych, Iryna",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.108/",
    pages = "2167--2187",
    ISBN = "979-8-89176-251-0",
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
